2023-02-20 09:44:11,603 - INFO - 
Config:
'add_train': False
'batch_size': 16
'cudadevice': 'cuda:1'
'cur_time': '2023_02_20_09_44_10'
'data_selected': ['A1'
'B1']
'do_figure_save': True
'do_log_print_to_screen': True
'do_log_save_to_file': True
'do_predict': False
'do_predict_roll': False
'do_train': True
'do_train_visualized': False
'dropout_rate': 0.2
'dynamic_data_root': 'input/csv/dynamic'
'dynamic_length': 2
'embedding_size': 256
'epoch': 100
'feature_columns': [0, 1, 3]
'figure_save_path': 'result/figure/'
'hidden_size': 256
'humansize': 9
'input_size': 3
'label_columns': [3]
'label_in_feature_index': [2]
'learning_rate': 5e-05
'log_save_path': 'result/logs/2023_02_20_09_44_10_pytorch/'
'lstm_layers': 2
'model_name': '.pth'
'model_postfix': {'pytorch': '.pth'}
'model_save_path': 'result/model/'
'name': 'A1,B1_No_Attention_hc_True_changeDe_RMLSE16_ds_fusion53'
'naturesize': 4
'only_dynamic': 0
'only_static_concat_dynamic': 0
'only_static_plus_dynamic': 0
'output_size': 1
'patience': 5
'predict_day': 3
'random_seed': 42
'roll_predict_day': 125
'shuffle_train_data': False
'static_data_path': 'input/csv/static/All_static.csv'
'static_human_column': [4, 5, 6, 7, 8, 9, 10, 11, 12]
'static_nature_column': [0, 1, 2, 3]
'time_step': 5
'train_data_rate': 0.95
'train_name': 'A1,B1'
'traindata': 'traindata'
'ttotrain': 1
'use_attention': False
'use_cuda': True
'use_static_embedding': True
'used_frame': 'pytorch'
'val_name': 'C1'
'valid_data_rate': 0.15
2023-02-20 09:44:14,830 - INFO - Epoch 0/100
2023-02-20 09:44:28,953 - INFO - The train loss is 0.578330. The valid loss is 0.391413.
2023-02-20 09:44:28,988 - INFO - Epoch 1/100
2023-02-20 09:44:38,401 - INFO - The train loss is 0.363127. The valid loss is 0.360994.
2023-02-20 09:44:38,464 - INFO - Epoch 2/100
2023-02-20 09:44:48,086 - INFO - The train loss is 0.315328. The valid loss is 0.284660.
2023-02-20 09:44:48,185 - INFO - Epoch 3/100
2023-02-20 09:44:55,998 - INFO - The train loss is 0.301290. The valid loss is 0.353224.
2023-02-20 09:44:55,998 - INFO - Epoch 4/100
2023-02-20 09:45:03,634 - INFO - The train loss is 0.285409. The valid loss is 0.281600.
2023-02-20 09:45:03,652 - INFO - Epoch 5/100
2023-02-20 09:45:13,639 - INFO - The train loss is 0.267602. The valid loss is 0.205257.
2023-02-20 09:45:13,735 - INFO - Epoch 6/100
2023-02-20 09:45:23,119 - INFO - The train loss is 0.297438. The valid loss is 0.425631.
2023-02-20 09:45:23,119 - INFO - Epoch 7/100
2023-02-20 09:45:33,376 - INFO - The train loss is 0.311027. The valid loss is 0.249704.
2023-02-20 09:45:33,377 - INFO - Epoch 8/100
2023-02-20 09:45:42,167 - INFO - The train loss is 0.254265. The valid loss is 0.235179.
2023-02-20 09:45:42,167 - INFO - Epoch 9/100
2023-02-20 09:45:53,103 - INFO - The train loss is 0.250966. The valid loss is 0.199218.
2023-02-20 09:45:53,163 - INFO - Epoch 10/100
2023-02-20 09:46:02,569 - INFO - The train loss is 0.245190. The valid loss is 0.181725.
2023-02-20 09:46:02,590 - INFO - Epoch 11/100
2023-02-20 09:46:11,914 - INFO - The train loss is 0.249912. The valid loss is 0.230484.
2023-02-20 09:46:11,915 - INFO - Epoch 12/100
2023-02-20 09:46:21,610 - INFO - The train loss is 0.239983. The valid loss is 0.191844.
2023-02-20 09:46:21,610 - INFO - Epoch 13/100
2023-02-20 09:46:31,596 - INFO - The train loss is 0.231936. The valid loss is 0.179194.
2023-02-20 09:46:31,705 - INFO - Epoch 14/100
2023-02-20 09:46:42,672 - INFO - The train loss is 0.270637. The valid loss is 0.368114.
2023-02-20 09:46:42,674 - INFO - Epoch 15/100
2023-02-20 09:46:52,227 - INFO - The train loss is 0.257600. The valid loss is 0.188405.
2023-02-20 09:46:52,228 - INFO - Epoch 16/100
2023-02-20 09:47:01,033 - INFO - The train loss is 0.237020. The valid loss is 0.202528.
2023-02-20 09:47:01,033 - INFO - Epoch 17/100
2023-02-20 09:47:10,539 - INFO - The train loss is 0.277769. The valid loss is 0.436117.
2023-02-20 09:47:10,539 - INFO - Epoch 18/100
2023-02-20 09:47:18,323 - INFO - The train loss is 0.289159. The valid loss is 0.212914.
2023-02-20 09:47:18,323 - INFO -  The training stops early in epoch 18
