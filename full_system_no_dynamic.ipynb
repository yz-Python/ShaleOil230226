{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 设定一些基本不会经常变化的参数\n",
    "class Config:  # 设定一些基本不会经常变化的参数\n",
    "    # 数据参数\n",
    "    feature_columns = list([0, 1, 3])  # 要作为feature的列，按原数据从0开始计算，也可以用list 如 [2,4,6,8] 设置\n",
    "    label_columns = [3]  # 要预测的列，按原数据从0开始计算, 如：将来可以同时预测第日产油量和日产气量\n",
    "    label_in_feature_index = (lambda x, y: [x.index(i) for i in y])(feature_columns, label_columns)\n",
    "    static_nature_column = list(range(0, 4))  ## 区分静态特征的两类 前四个属于地质参数和流体物性参数\t后面的属于工艺参数\n",
    "    dynamic_length = len(feature_columns) - len(label_columns)\n",
    "    static_human_column = list(range(4, 13))\n",
    "\n",
    "    input_size = len(feature_columns)\n",
    "    output_size = len(label_columns)\n",
    "\n",
    "    naturesize = len(static_nature_column)\n",
    "    humansize = len(static_human_column)\n",
    "\n",
    "    name = 'name'\n",
    "    roll_predict_day = 145  # 迭代预测未来天数\n",
    "    # 网络参数\n",
    "\n",
    "    save_data_cache = True  # 是否保存数据采样的结果\n",
    "    embedding_size = 256  # 送入LSTM前升维\n",
    "    hidden_size = 256  # LSTM的隐藏层大小，也是输出大小\n",
    "    lstm_layers = 2  # LSTM的堆叠层数\n",
    "    dropout_rate = 0.2  # dropout概率\n",
    "    time_step = 0  # 这个参数很重要，是设置用前多少天的数据来预测，也是LSTM的time step数\n",
    "    predict_day = 0  # 预测未来几天\n",
    "    # 训练参数\n",
    "    data_selected = []\n",
    "    do_train = True\n",
    "    do_predict = True\n",
    "    do_predict_roll = True\n",
    "    add_train = True  # 是否载入已有模型参数进行增量训练\n",
    "    shuffle_train_data = False  # 是否对训练数据做shuffle #如果把上一batch的hc传入下一batch，shuffle必须为False\n",
    "    use_cuda = True  # 是否使用GPU训练\n",
    "    cudadevice = 'cuda:0'  # 如果不使用gpu就改成 cudadevice='cpu',不改也行，模型在gpu没法用的情况下自动使用cpu\n",
    "    traindata = 'traindata'\n",
    "    train_data_rate = 0.67  # 训练数据占总体数据比例，测试数据就是 1-train_data_rate\n",
    "    valid_data_rate = 0.15  # 验证数据占训练数据比例，验证集在训练过程使用，为了做模型和参数选择\n",
    "\n",
    "    batch_size = 0\n",
    "    learning_rate = 0.00005\n",
    "    epoch = 200  # 整个训练集被训练多少遍，不考虑早停的前提下\n",
    "    patience = 5  # 训练多少epoch，验证集没提升就停掉\n",
    "    random_seed = 42  # 随机种子，保证可复现\n",
    "\n",
    "    # 框架参数\n",
    "    used_frame = \"pytorch\"  # 选择的深度学习框架，不同的框架模型保存后缀不一样\n",
    "    model_postfix = {\"pytorch\": \".pth\"}\n",
    "    model_name = model_postfix[used_frame]\n",
    "\n",
    "    # 路径参数\n",
    "    data_cache_path = 'cache'\n",
    "    static_data_path = 'input/csv/static/All_static.csv'\n",
    "    dynamic_data_root = 'input/csv/dynamic'\n",
    "    model_save_path = \"result/model/\"\n",
    "    figure_save_path = \"result/figure/\"\n",
    "    log_save_path = \"result/logs/\"\n",
    "    do_log_print_to_screen = True\n",
    "    do_log_save_to_file = True  # 是否将config和训练过程记录到log\n",
    "    do_figure_save = True\n",
    "    do_train_visualized = False  # 训练loss可视化，pytorch用visdom,留着下次使用\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)  # makedirs 递归创建目录\n",
    "    if not os.path.exists(figure_save_path):\n",
    "        os.mkdir(figure_save_path)\n",
    "    if do_train and (do_log_save_to_file or do_train_visualized):\n",
    "        cur_time = time.strftime(\"%Y_%m_%d_%H_%M_%S\", time.localtime())\n",
    "        log_save_path = log_save_path + cur_time + '_' + used_frame + \"/\"\n",
    "        os.makedirs(log_save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class RMSLEloss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, pred, actual):\n",
    "        return torch.sqrt(self.mse(torch.log(abs(pred + 1)), torch.log(abs(actual + 1))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 评估指标\n",
    "import numpy as np\n",
    "def mean_relative_error(predict, target):\n",
    "    error = target-predict\n",
    "    for i,j in  enumerate(error):\n",
    "        error[i] =abs(j)\n",
    "    mean_error = sum(error/target)/len(error)\n",
    "    return mean_error\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# ------------- 定义基本的模型框架 -------------------#\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from xpinyin import Pinyin\n",
    "# from model.full_model_no_dynamic import Net\n",
    "# from tool.loss.RMSLEloss import RMSLEloss\n",
    "import setproctitle\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# from tool.evaluate.mean_relative_error import mean_relative_error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 参数初始化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# argparse方便于命令行下输入参数，可以根据需要增加更多\n",
    "parser = argparse.ArgumentParser()\n",
    "# 如果在linux下面, 可以方便采用下面的参数输入\n",
    "# 在Windows环境下，可以单独修改参数，直接运行\n",
    "parser.add_argument(\"-b\", \"--batch_size\", default=16, type=int, help=\"batch size\")\n",
    "parser.add_argument(\"-e\", \"--epoch\", default=100, type=int, help=\"epochs num\")\n",
    "parser.add_argument(\"--ttotrain\", default=1, type=int, help=\"1代表训练,0代表测试\")\n",
    "parser.add_argument(\"--use_attention\", default=1, type=int, help=\"seq2seq_attention\")\n",
    "parser.add_argument(\"--use_static_embedding\", default=1, type=int, help=\"Inite,h0_c0\")\n",
    "\n",
    "parser.add_argument(\"--only_dynamic\", default=0, type=int, help=\"只用动态嵌入\")\n",
    "parser.add_argument(\"--only_static_concat_dynamic\", default=0, type=int, help=\"静态拼接动态降维\")\n",
    "parser.add_argument(\"--only_static_plus_dynamic\", default=0, type=int, help=\"静态和动态直接相加\")\n",
    "\n",
    "parser.add_argument(\"--roll_predict_day\", default=125, type=int, help=\"迭代预测天数\")  # 相当于让Decoder迭代多少次\n",
    "\n",
    "parser.add_argument(\"--predict_day\", default=3, type=int, help=\"Decoder预测天数\")\n",
    "parser.add_argument(\"--time_step\", default=5, type=int, help=\"Encoder读取天数\")\n",
    "\n",
    "parser.add_argument(\"--name\", default='A1', type=str, help=\"填写训练预设名称\")  # 可以填写'All' 或者'A1,B1,B590_1,B123'这种形式\n",
    "parser.add_argument(\"--val_name\", default='C1', type=str, help=\"填写测试预设名称\")  # 填写'C1'这种形式，暂时只支持测试1口井\n",
    "# only_dynamic only_static_concat_dynamic only_static_plus_dynamic\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_args(args=['--batch_size', '16',  '--epoch', '100', '--ttotrain', '1', '--use_attention', '1', '--use_static_embedding', '1', '--only_dynamic', '0', '--only_static_concat_dynamic', '0', '--only_static_plus_dynamic', '0', '--roll_predict_day', '125', '--predict_day', '3', '--time_step', '5', '--name', 'A1', '--val_name', 'C1'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:True\n",
      "已选择['A1']井训练\n"
     ]
    }
   ],
   "source": [
    "con = Config()\n",
    "for key in dir(args):  # dir(args) 函数获得args所有的属性\n",
    "    if not key.startswith(\"_\"):  # 去掉 args 自带属性，比如__name__等\n",
    "        setattr(con, key, getattr(args, key))  # 将属性值赋给Config\n",
    "# 建议window下，采用下面, 便于调试\n",
    "\n",
    "assert con.only_dynamic + con.only_static_concat_dynamic + con.only_static_plus_dynamic <= 1\n",
    "# 这几个参数做消融实验时候只能有同时存才1个或者都不存在，就代表动静态融合模块\n",
    "sign = True if con.ttotrain == 1 else False\n",
    "use_attention = '_No_Attention'\n",
    "use_static_embedding = '_hc_False'\n",
    "dynamic_fusion = '_ds_fusion'\n",
    "if con.only_dynamic == 1:\n",
    "    dynamic_fusion = '_dynamic_only'\n",
    "if con.only_static_concat_dynamic == 1:\n",
    "    dynamic_fusion = '_concat_only'\n",
    "if con.only_static_plus_dynamic == 1:\n",
    "    dynamic_fusion = '_plus_only'\n",
    "if con.use_attention == 1:\n",
    "    con.use_attention = True\n",
    "    use_attention = '_Attention'\n",
    "else:\n",
    "    con.use_attention = False\n",
    "if con.use_static_embedding == 1:\n",
    "    con.use_static_embedding = True\n",
    "    use_static_embedding = '_hc_True'\n",
    "else:\n",
    "    con.use_static_embedding = False\n",
    "con.train_name = con.name\n",
    "end_sign = '_RMLSE' + str(con.batch_size)\n",
    "con.name = con.name + use_attention + use_static_embedding + end_sign + dynamic_fusion + str(con.time_step) + str(\n",
    "    con.predict_day)\n",
    "# 设定名称，用于指示使用方法以及数据\n",
    "print('Train:' + str(sign))\n",
    "\n",
    "if sign == True:  # 表示训练\n",
    "    if ',' in con.train_name:\n",
    "        con.data_selected = con.train_name.split(',')\n",
    "        print('已选择{}井训练'.format(con.data_selected))\n",
    "    elif con.train_name == 'All':\n",
    "        con.data_selected = ['All']\n",
    "        print('已选择[{}]路径下所有井训练'.format(con.dynamic_data_root))\n",
    "    else:\n",
    "        con.data_selected = [con.train_name]  # 只输入单井情况\n",
    "        print('已选择{}井训练'.format(con.data_selected))\n",
    "    if not con.train_name:\n",
    "        assert ('井名输入错误')\n",
    "\n",
    "    con.train_data_rate = 0.95  # 设定数据用于训练的百分比\n",
    "    con.add_train = False  # 不加载权重进行训练\n",
    "    con.do_train = True\n",
    "    con.do_predict = False\n",
    "    con.do_predict_roll = False\n",
    "else:\n",
    "    con.data_selected = [con.val_name]\n",
    "    if not con.train_name:\n",
    "        assert ('井名输入错误')\n",
    "    con.train_data_rate = 0.1  # 取井剩下1-train_data_rate的百分比用于测试\n",
    "    con.do_train = False\n",
    "    con.do_predict = True\n",
    "    con.do_predict_roll = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:03:02 ] \n",
      "Config:\n",
      "'add_train': False\n",
      "'batch_size': 16\n",
      "'cudadevice': 'cuda:0'\n",
      "'cur_time': '2023_03_01_15_02_53'\n",
      "'data_cache_path': 'cache'\n",
      "'data_selected': ['A1']\n",
      "'do_figure_save': True\n",
      "'do_log_print_to_screen': True\n",
      "'do_log_save_to_file': True\n",
      "'do_predict': False\n",
      "'do_predict_roll': False\n",
      "'do_train': True\n",
      "'do_train_visualized': False\n",
      "'dropout_rate': 0.2\n",
      "'dynamic_data_root': 'input/csv/dynamic'\n",
      "'dynamic_length': 2\n",
      "'embedding_size': 256\n",
      "'epoch': 100\n",
      "'feature_columns': [0, 1, 3]\n",
      "'figure_save_path': 'result/figure/'\n",
      "'hidden_size': 256\n",
      "'humansize': 9\n",
      "'input_size': 3\n",
      "'label_columns': [3]\n",
      "'label_in_feature_index': [2]\n",
      "'learning_rate': 5e-05\n",
      "'log_save_path': 'result/logs/2023_03_01_15_02_53_pytorch/'\n",
      "'lstm_layers': 2\n",
      "'model_name': '.pth'\n",
      "'model_postfix': {'pytorch': '.pth'}\n",
      "'model_save_path': 'result/model/'\n",
      "'name': 'A1_Attention_hc_True_RMLSE16_ds_fusion53'\n",
      "'naturesize': 4\n",
      "'only_dynamic': 0\n",
      "'only_static_concat_dynamic': 0\n",
      "'only_static_plus_dynamic': 0\n",
      "'output_size': 1\n",
      "'patience': 5\n",
      "'predict_day': 3\n",
      "'random_seed': 42\n",
      "'roll_predict_day': 125\n",
      "'save_data_cache': True\n",
      "'shuffle_train_data': False\n",
      "'static_data_path': 'input/csv/static/All_static.csv'\n",
      "'static_human_column': [4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "'static_nature_column': [0, 1, 2, 3]\n",
      "'time_step': 5\n",
      "'train_data_rate': 0.95\n",
      "'train_name': 'A1'\n",
      "'traindata': 'traindata'\n",
      "'ttotrain': 1\n",
      "'use_attention': True\n",
      "'use_cuda': True\n",
      "'use_static_embedding': True\n",
      "'used_frame': 'pytorch'\n",
      "'val_name': 'C1'\n",
      "'valid_data_rate': 0.15\n",
      "动态数据加载完成\n",
      "静态数据加载完成\n"
     ]
    }
   ],
   "source": [
    "config = con\n",
    "np.random.seed(config.random_seed)  # 设置随机种子，保证可复现\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.模型训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "# 以5天预测3天为例子，输入的数据分别是：\n",
    "# 1-5天动态特征和产量，5-7天产量，6-8天产量，6-8天动态数据 这部分是训练集内容，验证集同理\n",
    "# 后面是人为静态参数和自然（地质）静态参数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1训练集验证集划分"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.do_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ------------- 数据处理部分 ------------------------#\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.full_static_data_path = self.normalize(config.static_data_path)  # 归一化静态数据，并获得静态数据地址用于自动选择\n",
    "\n",
    "        self.data, self.staticdata_human, self.staticdata_nature, self.data_column_name = self.read_data()\n",
    "\n",
    "        self.norm_human_data = self.staticdata_human\n",
    "        self.norm_nature_data = self.staticdata_nature\n",
    "        # self.data=self.data.astype('float')\n",
    "        self.train_num = []\n",
    "        # self.test_start_num = int(self.data_num * self.config.train_data_rate)\n",
    "        self.norm_data = []\n",
    "\n",
    "        self.std = []\n",
    "        self.mean = []\n",
    "        for i, data in enumerate(self.data):\n",
    "            self.data_num = data.shape[0]\n",
    "            train_num = int(self.data_num * self.config.train_data_rate)\n",
    "\n",
    "            mean = np.mean(data, axis=0)  # .reshape(self.data_num,1)\n",
    "            # 数据的均值和方差 axis =0在列上归一，axis=1在行上归一\n",
    "            std = np.std(data, axis=0)  # .reshape(self.data_num,1)\n",
    "\n",
    "            norm_data = (data - mean) / std  # 归一化，去量纲\n",
    "            self.norm_data.append(norm_data)\n",
    "            self.train_num.append(train_num)\n",
    "            self.mean.append(mean)\n",
    "            self.std.append(std)\n",
    "\n",
    "        # self.norm_nature_data.append(self.staticdata_nature[i].reshape(1,config.naturesize))\n",
    "\n",
    "        # self.norm_human_data.append(self.staticdata_human[i].reshape(1,config.humansize))\n",
    "\n",
    "    def normalize(self, static_path): # 标准化\n",
    "        Data = pd.read_csv(static_path)\n",
    "        cate = Data.columns.tolist()\n",
    "        Data = Data.values\n",
    "        feature = np.array(Data[:, :-1], dtype=float)\n",
    "        number = Data[:, -1:]\n",
    "        mean = np.mean(feature, axis=0)\n",
    "        std = np.std(feature, axis=0)\n",
    "        norm = (feature - mean) / std\n",
    "\n",
    "        norm_data = np.concatenate([norm, number], axis=1)\n",
    "        # print(norm_data)\n",
    "        df1 = pd.DataFrame(data=norm_data,\n",
    "                           columns=cate)\n",
    "        norm_static_file = static_path.replace('.csv', '_norm.csv')\n",
    "        df1.to_csv(norm_static_file, index=False)\n",
    "        return norm_static_file\n",
    "\n",
    "    def read_data(self):  # 读取初始数据\n",
    "        init_data = []\n",
    "        data_column_name = []\n",
    "        all_data_name = []\n",
    "        for name in os.listdir(self.config.dynamic_data_root):\n",
    "            if \".csv\" in name:\n",
    "                all_data_name.append(name)  # 带有csv后缀,遍历所有数据集的名称\n",
    "        for name in self.config.data_selected:  # 从设定的井编号读取数据，数据形式为['All']或者['A1','C1']\n",
    "            if name == 'All':  # name是自己输入决定使用的井，data_name是遍历得到的所有数据集的csv名称列表\n",
    "                assert len(self.config.data_selected) == 1  # 保证输入只有All，不能All 和A1，B1一起写\n",
    "                for data_name in all_data_name:  # 读取全部井训练数据\n",
    "                    data = pd.read_csv(os.path.join(self.config.dynamic_data_root, data_name),\n",
    "                                       usecols=self.config.feature_columns)\n",
    "                    init_data.append(data.values)\n",
    "                    data_column_name.append(data.columns.tolist())\n",
    "            else:\n",
    "                name_csv = name + \".csv\"  # A1 → A1.csv\n",
    "                try:\n",
    "                    data = pd.read_csv(os.path.join(self.config.dynamic_data_root, name_csv),\n",
    "                                       usecols=self.config.feature_columns)  # 读取name井训练数据\n",
    "                    init_data.append(data.values)\n",
    "                    data_column_name.append(data.columns.tolist())\n",
    "                except ValueError:\n",
    "                    print('选取的{}井与读取到的数据集井不匹配，检查数据集或检查data_selected参数'.format(name))\n",
    "        print('动态数据加载完成')\n",
    "        data_selected = self.config.data_selected\n",
    "        if data_selected[0] == 'All':\n",
    "            data_selected = []\n",
    "            for name in all_data_name:\n",
    "                data_selected.append(name.replace('.csv', ''))\n",
    "        init_static_data = np.array(\n",
    "            self.select_static_data(dynamic_name_list=data_selected, norm_static_file=self.full_static_data_path))\n",
    "        init_static_human_data = init_static_data[:, :,\n",
    "                                 self.config.static_human_column].tolist()  # self.select_static_data(dynamic_name_list=data_selected,norm_static_file=self.full_static_data_path,\n",
    "        # cols=self.config.static_human_column)\n",
    "        init_static_nature_data = init_static_data[:, :, self.config.static_nature_column].tolist()\n",
    "        return init_data, init_static_human_data, init_static_nature_data, data_column_name\n",
    "        # .columns.tolist() 是获取列名\n",
    "\n",
    "    def select_static_data(self, dynamic_name_list, norm_static_file):  # 动态数据输入形式：['A1','B1','C1']\n",
    "        static_data = []\n",
    "        # print(norm_static_file)\n",
    "        Data = pd.read_csv(norm_static_file)\n",
    "        Data = Data.values\n",
    "        feature = np.array(Data[:, :-1], dtype=float)\n",
    "        Name = Data[:, -1:].tolist()\n",
    "        try:\n",
    "            for dynamic_name in dynamic_name_list:\n",
    "                index = Name.index([dynamic_name])\n",
    "                static_data.append(feature[index:index + 1, :])\n",
    "            # print(static_data)\n",
    "        except ValueError:\n",
    "            print('{}井不匹配静态数据，检查是否确实静态数据或者井编号不对应'.format(dynamic_name))\n",
    "            os.kill()\n",
    "        print('静态数据加载完成')\n",
    "        return (static_data)\n",
    "\n",
    "    def get_train_and_valid_data(self): # 训练集测试集划分\n",
    "        full_train_data, full_valid_data, full_train_label, full_valid_label = [], [], [], []\n",
    "        for j in range(len(self.norm_data)):\n",
    "            data_ind = self.norm_data[j]\n",
    "            print(data_ind.shape)\n",
    "            feature_data = data_ind[:self.train_num[j]]\n",
    "            label_data = data_ind[:self.train_num[j],\n",
    "                         self.config.label_in_feature_index]\n",
    "            norm_nature_data = self.norm_nature_data[j]\n",
    "            norm_human_data = self.norm_human_data[j]\n",
    "\n",
    "            static_nature_data = np.array([norm_nature_data for i in range(self.config.time_step)])\n",
    "            static_nature_data = static_nature_data.reshape(static_nature_data.shape[0], static_nature_data.shape[2])\n",
    "            static_human_data = np.array(\n",
    "                [norm_human_data for i in range(self.config.time_step)])  # 静态数据复制5遍，因为要和5天预测3天维度的encoder输入数据对应上\n",
    "            static_human_data = static_human_data.reshape(static_human_data.shape[0],\n",
    "                                                          static_human_data.shape[2])  # 这里实现的是静态数据添加时间信息\n",
    "\n",
    "            embed = np.eye(self.config.time_step)  # [1 0 0]\n",
    "            static_nature_data = np.append(static_nature_data, embed, axis=1)  # [0 1 0]\n",
    "            static_human_data = np.append(static_human_data, embed, axis=1)  # [0 0 1]  拼接进去作为One-Hot编码 表示时间顺序\n",
    "\n",
    "            length = self.config.time_step + self.config.predict_day  # 5+3=8\n",
    "            # print(length)\n",
    "            feature = [feature_data[start_index + i * (length): start_index + (i + 1) * (length)]\n",
    "                       # 训练部分按照错位进行采样得到的形式,包含油嘴油压产量\n",
    "                       for start_index in range(length)  # 这里是动态特征部分1-5天预测6-8,2-6天预测7-9这样一致往后推\n",
    "                       for i in range((self.train_num[j] - start_index) // (length))]  # 取得数据是1-8天，2-9天这样顺序，这样堆叠下去\n",
    "            label = [label_data[start_index + i * length: start_index + (i + 1) * length]  # 标签部分处理方法相同，仅有产量\n",
    "                     for start_index in range(length)\n",
    "                     for i in range((self.train_num[j] - start_index) // length)]\n",
    "            feature, label = np.array(feature), np.array(label)\n",
    "            print(feature.shape, label.shape)\n",
    "\n",
    "            train_data, valid_data, train_label, valid_label = train_test_split(feature, label,\n",
    "                                                                                test_size=self.config.valid_data_rate,\n",
    "                                                                                random_state=self.config.random_seed,\n",
    "                                                                                shuffle=self.config.shuffle_train_data)  # 划分训练和验证集\n",
    "            s_h_train = np.array([static_human_data for k in range(len(train_data))])  # s表示静态数据，h表示人工参数\n",
    "            s_n_train = np.array([static_nature_data for k in range(len(train_data))])  # ，n表示自然（地质）参数\n",
    "            s_h_valid = np.array([static_human_data for k in range(len(valid_data))])\n",
    "            s_n_valid = np.array([static_nature_data for k in range(len(valid_data))])\n",
    "            if j == 0:\n",
    "                full_train_data, full_valid_data, full_train_label, full_valid_label = train_data, valid_data, train_label, valid_label\n",
    "                full_s_h_train, full_s_n_train, full_s_h_valid, full_s_n_valid = s_h_train, s_n_train, s_h_valid, s_n_valid\n",
    "            else:\n",
    "                full_train_data, full_valid_data = np.concatenate([full_train_data, train_data],\n",
    "                                                                  axis=0), np.concatenate([full_valid_data, valid_data],\n",
    "                                                                                          axis=0)\n",
    "                full_train_label, full_valid_label = np.concatenate([full_train_label, train_label],\n",
    "                                                                    axis=0), np.concatenate(\n",
    "                    [full_valid_label, valid_label], axis=0)\n",
    "                full_s_h_train = np.concatenate([full_s_h_train, s_h_train], axis=0)\n",
    "                full_s_n_train = np.concatenate([full_s_n_train, s_n_train], axis=0)\n",
    "                full_s_h_valid = np.concatenate([full_s_h_valid, s_h_valid], axis=0)\n",
    "                full_s_n_valid = np.concatenate([full_s_n_valid, s_n_valid], axis=0)  # 这里是将多井数据拼在一起，用于一起训练\n",
    "\n",
    "        # 此处分别对训练集和测试集进行对应输入采样\n",
    "        # full_train_data是一个[xxx,8,3]的数据 full_train_label是一个[xxx，8,1]的数据\n",
    "        encoder_train = full_train_data[:, :self.config.time_step, :]  # 取1-5天的动态数据和产量，油嘴油压产量\n",
    "        decoder_train = full_train_label[:, self.config.time_step - 1:length - 1,\n",
    "                        :]  # 取5-7天的产量，最初是为了用于teacherforcing的进行训练，后来没有用teacherforcing，在实际的验证集没有用到这部分数据\n",
    "        label_train = full_train_label[:, self.config.time_step:length, :]  # 取6-8天的产量，作为标签\n",
    "        # dynamic_train = full_train_data[:,self.config.time_step:length,:self.config.dynamic_length] #取5-8天的动态数据，油嘴油压\n",
    "\n",
    "        encoder_valid = full_valid_data[:, :self.config.time_step, :]  # 验证集，同上\n",
    "        decoder_valid = full_valid_label[:, self.config.time_step - 1:length - 1, :]\n",
    "        label_valid = full_valid_label[:, self.config.time_step:length, :]\n",
    "        # dynamic_valid = full_valid_data[:,self.config.time_step:length,:self.config.dynamic_length]\n",
    "        if self.config.save_data_cache == True:\n",
    "            self.save_cache(encoder_train, label_train, category='train')\n",
    "            self.save_cache(encoder_valid, label_valid, category='valid')  # 保存中间数据采样结果，注意数据都是经过归一化的\n",
    "\n",
    "        return encoder_train, decoder_train, label_train, \\\n",
    "               encoder_valid, decoder_valid, label_valid, \\\n",
    "               full_s_h_train, full_s_n_train, full_s_h_valid, full_s_n_valid\n",
    "\n",
    "    def get_test_data(self, roll, return_label_data=False):\n",
    "        norm_data = self.norm_data[0]  # 测试只考虑单井测试情况。\n",
    "        train_num = self.train_num[0]\n",
    "        feature_data = norm_data[train_num:]  # 测试样本train_num需要被设定为0\n",
    "        time_step = self.config.time_step  # 防止time_step大于测试集数量\n",
    "        step_for_no_roll = ((feature_data.shape[0] - time_step) // self.config.predict_day) + 1\n",
    "        print(step_for_no_roll)\n",
    "        norm_nature_data = self.norm_nature_data[0]\n",
    "        norm_human_data = self.norm_human_data[0]\n",
    "        static_nature_data = np.array([norm_nature_data for i in range(self.config.time_step)])\n",
    "        static_nature_data = static_nature_data.reshape(static_nature_data.shape[0], static_nature_data.shape[2])\n",
    "        static_human_data = np.array([norm_human_data for i in range(self.config.time_step)])  # 数据形式有点奇怪，不知道为什么，反正调一下\n",
    "        static_human_data = static_human_data.reshape(static_human_data.shape[0], static_human_data.shape[2])\n",
    "\n",
    "        embed = np.eye(self.config.time_step)  # [1 0 0]\n",
    "        static_nature_data = np.append(static_nature_data, embed, axis=1)  # [0 1 0]\n",
    "        static_human_data = np.append(static_human_data, embed, axis=1)  # [0 0 1]  拼接进去作为One-Hot编码\n",
    "\n",
    "        # 在滚动测试数据中，采样方式按照Predict_day连续进行错位采样 1-5天预测6-8天，4-8天预测9-11天，与训练数据采样方式不同\n",
    "        # 迭代测试数据仅采样1-5天数据，预测天数按照roll_predict_day决定\n",
    "        if roll:  # roll表示迭代预测\n",
    "            encoder_test = [feature_data[: time_step]]\n",
    "            decoder_test = [feature_data[time_step:]]\n",
    "            # decoder_test= np.array(decoder_test)\n",
    "            # decoder_test = decoder_test[:,:,:self.config.dynamic_length]\n",
    "            return np.array(encoder_test), static_human_data, static_nature_data\n",
    "\n",
    "        if not roll:  # 滚动预测\n",
    "            new_encoder_test = []\n",
    "            if self.config.time_step >= self.config.predict_day:  # 采样方式按照Predict_day连续进行错位采样 1-5天预测6-8天，4-8天预测9-11天，与训练数据采样方式不同\n",
    "                encoder_test = [feature_data[i * self.config.predict_day:  time_step + i * self.config.predict_day]  #\n",
    "                                for i in range(step_for_no_roll)]  # 按照1-5天，4-8天。。。这样的顺序采样encoder输入样本\n",
    "\n",
    "            if self.config.time_step < self.config.predict_day:  # 基本用不上这种情况，不会出现预测天数大于输入天数的滚动预测\n",
    "                middle_day = self.config.predict_day\n",
    "                encoder_test = [feature_data[i * middle_day:  time_step + i * middle_day]\n",
    "                                for i in range(step_for_no_roll)]  #\n",
    "\n",
    "            for i in encoder_test:\n",
    "                if len(i) == self.config.time_step:\n",
    "                    new_encoder_test.append(i)  # 去除编码器输入数据不够的样本\n",
    "\n",
    "        #     new_decoder_test =[]\n",
    "        #     decoder_test = [feature_data[  time_step+i*self.config.predict_day :  time_step+(i+1)*self.config.predict_day]\n",
    "        #         for i in range(step_for_no_roll)] #按照6-8天，9-11天。。。这样的顺序采样decoder输入样本\n",
    "\n",
    "        #     for i in decoder_test :\n",
    "        #         if len(i) ==self.config.predict_day:\n",
    "        #             new_decoder_test.append(i)\n",
    "\n",
    "        # new_decoder_test =np.array(new_decoder_test)\n",
    "        # new_decoder_test = new_decoder_test[:,:,:self.config.dynamic_length] #decoder只保留油嘴油压数据\n",
    "        encoder_test = np.array(new_encoder_test)\n",
    "        if self.config.save_data_cache == True:\n",
    "            self.save_cache(encoder_test, None, category='test')\n",
    "        # decoder_test =new_decoder_test\n",
    "\n",
    "        # final_len =min(len(encoder_test),len(decoder_test))\n",
    "        # encoder_test = encoder_test[:final_len,:,:]   #保证测试样本数目对应\n",
    "        # decoder_test = decoder_test[:final_len,:,:]\n",
    "\n",
    "        return encoder_test, static_human_data, static_nature_data\n",
    "\n",
    "    def save_cache(self, encoder, label, category):\n",
    "        import shutil\n",
    "        assert category in ['train', 'valid', 'test']\n",
    "        cache_path = os.path.join(self.config.data_cache_path, category)  # 获得根目录\n",
    "        encoder_path = os.path.join(cache_path, 'encoder_input')\n",
    "        shutil.rmtree(encoder_path, ignore_errors=True)  # 清空目录\n",
    "        os.makedirs(encoder_path)  # 生成目录\n",
    "\n",
    "        for number in range(encoder.shape[0]):  # 如果只是测试集,就只需要保存encoder\n",
    "            data_encoder = encoder[number]\n",
    "            np.savetxt(os.path.join(encoder_path, str(number) + \".csv\"), data_encoder, delimiter=',')\n",
    "\n",
    "        if category in ['train', 'valid']:\n",
    "            label_path = os.path.join(cache_path, 'label')  # 如果训练过程，还需要保存验证集\n",
    "            shutil.rmtree(label_path, ignore_errors=True)\n",
    "            os.makedirs(label_path)\n",
    "            for number_label in range(encoder.shape[0]):\n",
    "                data_label = label[number_label]\n",
    "                np.savetxt(os.path.join(label_path, str(number_label) + \".csv\"), data_label, delimiter=',')\n",
    "\n",
    "\n",
    "## 保存日志\n",
    "import logging\n",
    "import sys\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "frame = \"pytorch\"\n",
    "\n",
    "\n",
    "def load_logger(config):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level=logging.DEBUG)\n",
    "\n",
    "    # StreamHandler\n",
    "    if config.do_log_print_to_screen:\n",
    "        stream_handler = logging.StreamHandler(sys.stdout)\n",
    "        stream_handler.setLevel(level=logging.INFO)\n",
    "        formatter = logging.Formatter(datefmt='%Y/%m/%d %H:%M:%S',\n",
    "                                      fmt='[ %(asctime)s ] %(message)s')\n",
    "        stream_handler.setFormatter(formatter)\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "    # FileHandler\n",
    "    if config.do_log_save_to_file:\n",
    "        file_handler = RotatingFileHandler(config.log_save_path + \"out.log\", maxBytes=1024000, backupCount=5)\n",
    "        file_handler.setLevel(level=logging.INFO)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "        # 把config信息也记录到log 文件中\n",
    "        config_dict = {}\n",
    "        for key in dir(config):\n",
    "            if not key.startswith(\"_\"):\n",
    "                config_dict[key] = getattr(config, key)\n",
    "        config_str = str(config_dict)\n",
    "        config_list = config_str[1:-1].split(\", '\")\n",
    "        config_save_str = \"\\nConfig:\\n\" + \"\\n'\".join(config_list)\n",
    "        logger.info(config_save_str)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "def draw(config: Config, origin_data: Data, logger, predict_norm_data: np.ndarray, name: str):\n",
    "    # label_data = origin_data.data[origin_data.train_num + origin_data.start_num_in_test :\n",
    "    #                     origin_data.train_num + origin_data.start_num_in_test +config.roll_predict_day,\n",
    "    #                                         config.label_in_feature_index\n",
    "    label_len = len(predict_norm_data)\n",
    "    calculate_label_data = origin_data.data[0][origin_data.train_num[0] + config.time_step:\n",
    "                                               origin_data.train_num[0] + label_len + config.time_step,\n",
    "                           config.label_in_feature_index]\n",
    "    label_data = origin_data.data[0][origin_data.train_num[0]:, config.label_in_feature_index]\n",
    "    actual_len = len(calculate_label_data)\n",
    "    print(label_data.shape)\n",
    "\n",
    "    predict_data = predict_norm_data * origin_data.std[0][config.label_in_feature_index] + \\\n",
    "                   origin_data.mean[0][config.label_in_feature_index]  # 通过保存的均值和方差还原数据, 这个很重要\n",
    "\n",
    "    label_name = [origin_data.data_column_name[0][i] for i in config.label_in_feature_index]\n",
    "    label_column_num = len(config.label_columns)\n",
    "    p = Pinyin()\n",
    "    label_name_pinyin = []\n",
    "    for i in range(len(label_name)):\n",
    "        label_name_pinyin.append(p.get_pinyin(label_name[i]))\n",
    "    # label 和 predict 是错开config.predict_day天的数据的\n",
    "    # 下面是两种norm后的loss的计算方式，结果是一样的，可以简单手推一下\n",
    "    # label_norm_data = origin_data.norm_data[origin_data.train_num + origin_data.start_num_in_test:,\n",
    "    #              config.label_in_feature_index]\n",
    "    # loss_norm = np.mean((label_norm_data[config.predict_day:] - predict_norm_data[:-config.predict_day]) ** 2, axis=0)\n",
    "    # logger.info(\"The mean squared error of {} is \".format(label_name) + str(loss_norm))\n",
    "    # mean_squared_error ,mean_absolute_error ,r2_score\n",
    "    mse = mean_squared_error(calculate_label_data[:], predict_data[:actual_len])\n",
    "    mae = mean_absolute_error(calculate_label_data[:], predict_data[:actual_len])\n",
    "    r2 = r2_score(calculate_label_data[:], predict_data[:actual_len])\n",
    "    mre = mean_relative_error(calculate_label_data[:], predict_data[:actual_len])\n",
    "    # loss = np.sum((calculate_label_data[:] - predict_data[:] ) ** 2)/len(predict_data)\n",
    "    # loss_norm = loss#/(origin_data.std[0][config.label_in_feature_index] ** 2)\n",
    "    logger.info(\"The mean relative error of {} is \".format(label_name_pinyin) + str(mre))\n",
    "    logger.info(\"The mean squared error of {} is \".format(label_name_pinyin) + str(mse))\n",
    "    logger.info(\"The mean average error of {} is \".format(label_name_pinyin) + str(mae))\n",
    "    logger.info(\"The R^2 of {} is \".format(label_name_pinyin) + str(r2))\n",
    "\n",
    "    # label_train = range(origin_data.data_num - origin_data.train_num - origin_data.start_num_in_test)\n",
    "    label_train = range(len(label_data))\n",
    "    predict_X = range(len(predict_data))\n",
    "    predict_X = [x + config.time_step for x in predict_X]  # 错开5天画图\n",
    "    '''\n",
    "    这里需要修改代码，保存预测值和标注值\n",
    "    '''\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "    if True:  # not sys.platform.startswith('linux'):    # 无桌面的Linux下无法输出，如果是有桌面的Linux，如Ubuntu，可去掉这一行\n",
    "        for i in range(label_column_num):\n",
    "            plt.figure(i + 1)\n",
    "            # plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]\n",
    "            # 预测数据绘制\n",
    "            # 这里是中文的设置, 没有这个设置\"日产油量无法显示\"\n",
    "            plt.plot(label_train, label_data[:, i], label='label')\n",
    "            plt.plot(predict_X, predict_data[:, i],\n",
    "                     label='predict')  # plt.title(\"Predict {}  with {}\".format(label_name[i], config.used_frame))\n",
    "            plt.title(config.train_name + \"预测\" + config.val_name)\n",
    "            plt.legend()\n",
    "            logger.info(\n",
    "                \"The predicted {} for the next {} day(s) is: \".format(label_name_pinyin[i], config.predict_day) +\n",
    "                str(np.squeeze(predict_data[-config.predict_day:, i])))\n",
    "            if config.do_figure_save:\n",
    "                # print(config.figure_save_path+\"add_stastic_{}predict_{}_with_{}.png\".format(config.continue_flag, label_name_pinyin[i], config.used_frame))\n",
    "                plt.savefig(config.figure_save_path + \"{}.png\".format(config.val_name + '_val_' + config.name + name))\n",
    "        plt.show()\n",
    "    df1 = pd.DataFrame(data=predict_data, columns=[name])\n",
    "    df1.to_csv('result.csv', index=False)\n",
    "logger = load_logger(config)  # 日志"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 获取数据  准备划分验证集测试集\n",
    "if config.do_train:\n",
    "\n",
    "    data_gainer = Data(config)  # 类初始化\n",
    "    encoder_train, decoder_train, label_train, encoder_valid, decoder_valid, label_valid, \\\n",
    "    full_s_h_train, full_s_n_train, full_s_h_valid, full_s_n_valid = data_gainer.get_train_and_valid_data()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(515, 3)\n",
      "(482, 8, 3) (482, 8, 1)\n",
      "torch.Size([409, 5, 3]) torch.Size([409, 3, 1]) torch.Size([409, 3, 1])\n",
      "torch.Size([73, 5, 3]) torch.Size([73, 3, 1]) torch.Size([73, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 划分验证集测试集  准备训练模型需要的数据\n",
    "if config.do_train:\n",
    "\n",
    "    device = torch.device(config.cudadevice if config.use_cuda and torch.cuda.is_available() else \"cpu\")  # CPU训练还是GPU\n",
    "\n",
    "    if config.do_train_visualized:\n",
    "        import visdom\n",
    "        vis = visdom.Visdom(env='model_pytorch')\n",
    "    # encoder_train, decoder_train, label_train, encoder_valid, decoder_valid, label_valid, \\\n",
    "    # full_s_h_train, full_s_n_train, full_s_h_valid, full_s_n_valid = train_and_valid_data\n",
    "    # decoder_train是在teacherforcing下送入Decoder的t-1时刻产量，dynamic_train是t时刻影响产量的动态数据\n",
    "    full_s_h_train, full_s_n_train = torch.from_numpy(full_s_h_train).float(), torch.from_numpy(full_s_n_train).float()\n",
    "    full_s_h_valid, full_s_n_valid = torch.from_numpy(full_s_h_valid).float(), torch.from_numpy(full_s_n_valid).float()\n",
    "    encoder_train, decoder_train, label_train = torch.from_numpy(encoder_train).float(), torch.from_numpy(\n",
    "        decoder_train).float(), torch.from_numpy(label_train).float()  # 先转为Tensor\n",
    "    print(encoder_train.shape, decoder_train.shape, label_train.shape)\n",
    "    train_loader = DataLoader(TensorDataset(encoder_train, decoder_train, label_train, full_s_h_train, full_s_n_train),\n",
    "                              batch_size=config.batch_size)  # DataLoader可自动生成可训练的batch数据\n",
    "\n",
    "    encoder_valid, decoder_valid, label_valid = torch.from_numpy(encoder_valid).float(), torch.from_numpy(\n",
    "        decoder_valid).float(), torch.from_numpy(label_valid).float()\n",
    "    valid_loader = DataLoader(TensorDataset(encoder_valid, decoder_valid, label_valid, full_s_h_valid, full_s_n_valid),\n",
    "                              batch_size=config.batch_size)\n",
    "    print(encoder_valid.shape, decoder_valid.shape, label_valid.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 训练集测试集划分输出 这也是下部分输入\n",
    "# 训练集：train_loader\n",
    "# 验证集：valid_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2模型训练模块"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 模型\n",
    "import torch\n",
    "from torch.nn import Module, LSTM, Linear, Conv1d, Softmax, ReLU, BatchNorm2d, LayerNorm, ModuleDict\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.LSTM = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=dropout_rate)\n",
    "\n",
    "    def forward(self, src, h_0, c_0):\n",
    "        batch, _, _ = src.shape\n",
    "        if h_0 == None:\n",
    "            outputs, (h, c) = self.LSTM(src, )\n",
    "        else:\n",
    "            h_0 = h_0[:, :batch, :].contiguous()\n",
    "            c_0 = c_0[:, :batch, :].contiguous()\n",
    "            outputs, (h, c) = self.LSTM(src, (h_0, c_0))\n",
    "\n",
    "        return outputs.transpose(0, 1), h, c\n",
    "\n",
    "    '''\n",
    "    output:最后一层LSTM的每个隐藏状态h        [batch_size,5,256]\n",
    "    h:每一层最后一个隐藏状态ht    [2,batch_size,256] 一共2层LSTM\n",
    "    c:每一层最后一个细胞状态ct\n",
    "    模型输入1-5天的动态数据，输出5天的编码隐藏层和最后一天的隐藏层h，细胞层c\n",
    "    '''\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim, bidirectional=False):  # 即encoder和decoder的输出维度\n",
    "        super(Attention, self).__init__()\n",
    "        # 双向的话，enc_hid_dim要乘2\n",
    "\n",
    "        self.q = nn.Linear(enc_hid_dim, dec_hid_dim, bias=False)  # 不要偏置，做一个线性变换\n",
    "        self.k = nn.Linear(enc_hid_dim, dec_hid_dim, bias=False)\n",
    "        self.v = nn.Linear(enc_hid_dim, dec_hid_dim, bias=False)  # 不要偏置，做一个线性变换\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.down = nn.Linear(dec_hid_dim * 2, dec_hid_dim, bias=False)\n",
    "\n",
    "    def forward(self, h, enc_out):\n",
    "        # torch.Size([2, 8, 256]) torch.Size([5, 8, 256]) 2是层数，8是batch,5是Encoder天数\n",
    "        h = h.permute(1, 0, 2)  # 取h化为[8,2,256]\n",
    "        enc_out = enc_out.permute(1, 0, 2)  # 化为[8,5,256]\n",
    "        q = self.q(h)\n",
    "        k = self.k(enc_out)\n",
    "        v = self.v(enc_out)\n",
    "        qk = torch.einsum('bij,bkj->bik', q, k)  # [8,2,10]\n",
    "        att = self.softmax(qk)\n",
    "        att_out = torch.einsum('bij,bjk->bik', att, v)  # [8,2,256]\n",
    "        att_out = torch.cat([h, att_out], dim=2)\n",
    "        out = self.down(att_out)\n",
    "        out = out.permute(1, 0, 2).contiguous()\n",
    "\n",
    "        return out\n",
    "\n",
    "    '''\n",
    "    输入h为[2,batch_size,256]，为Decoder部分送入的一日隐藏状态，2是LSTM层数\n",
    "    对Encoder5天每一天的隐藏层进行加权求和，得到注意力结果，输出维度不变\n",
    "    注意力机制输入上一时刻的隐藏层h和encoder5天的编码隐藏层\n",
    "    输出为经过注意力加权的上一时刻隐藏层隐藏层h\n",
    "    '''\n",
    "\n",
    "\n",
    "class ATT_Decoder(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size, num_layers, attention, config, embedding_dim, dropout_rate,\n",
    "                 dynamicDe_num, bidirectional=False):\n",
    "        super(ATT_Decoder, self).__init__()\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # self.attention1 = Attention(config.hidden_size, config.hidden_size)\n",
    "        self.attention2 = Attention(config.hidden_size, config.hidden_size)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.fc11 = nn.Linear(1, embedding_dim)\n",
    "        # self.fc12 = nn.Linear(dynamicDe_num,embedding_dim)\n",
    "        # self.fc1 = nn.Linear(2*embedding_dim,embedding_dim)\n",
    "        # self.fc2 = nn.Linear(self.enc_hidden_size+self.dec_hidden_size+self.embedding_dim,1)\n",
    "        self.fc3 = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.dec_hidden_size,\n",
    "                             num_layers=self.num_layers,\n",
    "                             batch_first=True, dropout=dropout_rate)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.dec_hidden_size,\n",
    "                             num_layers=self.num_layers,\n",
    "                             batch_first=True, dropout=dropout_rate)\n",
    "\n",
    "    def forward(self, dec_input, h1, c1, h2, c2, enc_output):\n",
    "        # dec_input：torch.Size([1, B, 1])  dyanmic_dec_input:torch.Size([1, B, 2])\n",
    "        # h1:torch.Size([2, B, 256]) c1:torch.Size([2, B, 256])\n",
    "        # h2：torch.Size([2, B, 256])  c2：torch.Size([2, B, 256])\n",
    "        # enc_output: torch.Size([5, B, 256])\n",
    "        #\n",
    "\n",
    "        embedded = self.fc11(dec_input.transpose(0, 1))  # [batchsize，seqlen，embeddingsize]\n",
    "        # Decoderinput是上一时刻的产量\n",
    "        # embedded2 = self.fc12(dyanmic_dec_input.transpose(0,1))     #是当前时刻的动态影响因素，需要进行特征拼接\n",
    "        # embedded = torch.cat([embedded1,embedded2],2)\n",
    "        # embedded = self.fc1(embedded)   #变成：# embedded = [ batch_size, 1, emb_dim]\n",
    "        # att_h1 = self.attention1(h1, enc_output)\n",
    "        embedded, (h1, c1) = self.lstm1(embedded, (h1, c1))  # 先进行一次Decoder编码，再去做attention\n",
    "\n",
    "        # att_h2 = self.attention2(h2, enc_output)\n",
    "        # lstm_input = embedded\n",
    "        # dec_output, (dec_h, dec_c) = self.lstm2(lstm_input, (att_h2, c2))\n",
    "\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        att_embedded = self.attention2(embedded, enc_output)\n",
    "        # 用decoder的当前状态编码后的h和encoder的所有时刻的状态enc_output的隐藏侧h计算注意力权重。\n",
    "        att_embedded = att_embedded.permute(1, 0, 2)\n",
    "        dec_output, (h2, c2) = self.lstm2(att_embedded, (h2, c2))\n",
    "\n",
    "        pred = self.fc3(dec_output)\n",
    "        pred = pred.transpose(0, 1)\n",
    "        return pred, h1, c1, h2, c2\n",
    "\n",
    "    '''\n",
    "    输入t-1时刻的产量信息，输入t时刻的动态信息（油嘴油压），以及t-1时刻的隐藏层h1,h2和细胞层\n",
    "    输出t时刻的产量预测值\n",
    "    '''\n",
    "\n",
    "\n",
    "class No_ATT_Decoder(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size, num_layers, attention, embedding_dim, dropout_rate,\n",
    "                 dynamicDe_num, bidirectional=False):\n",
    "        super(No_ATT_Decoder, self).__init__()\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.attention = attention\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.fc11 = nn.Linear(1, embedding_dim)\n",
    "        # self.fc12 = nn.Linear(dynamicDe_num,embedding_dim)\n",
    "        # self.fc1 = nn.Linear(2*embedding_dim,embedding_dim)\n",
    "        # self.fc2 = nn.Linear(self.enc_hidden_size+self.dec_hidden_size+self.embedding_dim,1)\n",
    "        self.fc3 = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.dec_hidden_size, num_layers=self.num_layers,\n",
    "                            dropout=dropout_rate)\n",
    "\n",
    "    def forward(self, dec_input, h, c, enc_output):\n",
    "        embedded = self.fc11(dec_input.transpose(0, 1))  # 应该是[batchsize，seqlen，embeddingsize]\n",
    "        # embedded2 = self.fc12(dyanmic_dec_input.transpose(0,1))\n",
    "        # embedded = torch.cat([embedded1,embedded2],2)\n",
    "        # embedded = self.fc1(embedded)\n",
    "        enc_output = enc_output.transpose(0, 1)  # enc_output = [batch_size, src_len, enc_hid_dim]\n",
    "        lstm_input = embedded.transpose(0,\n",
    "                                        1)  # torch.cat((embedded,att_c), dim=2)   #lstm_input = [1, batch_size, enc_hid_dim + emb_dim]\n",
    "        dec_output, (dec_h, dec_c) = self.lstm(lstm_input, (h, c))\n",
    "        pred = self.fc3(dec_output)\n",
    "        return pred, dec_h, dec_c\n",
    "\n",
    "\n",
    "class ECA(Module):  # 动静态特征融合模块\n",
    "    def __init__(self, config):\n",
    "        super(ECA, self).__init__()\n",
    "\n",
    "        x_size = config.input_size  # 3\n",
    "        self.x_size = x_size\n",
    "        self.dynamic_size = x_size - 1  # 动态因素，和产量分开\n",
    "        humansize = config.humansize  # 9\n",
    "        naturesize = config.naturesize  # 4\n",
    "        embeddingsize = config.embedding_size\n",
    "\n",
    "        self.full_range = np.arange(embeddingsize * 3)\n",
    "        np.random.shuffle(self.full_range)\n",
    "        self.shuffle_list = self.full_range  # shuffle过后\n",
    "\n",
    "        self.humanlinear = Linear(humansize + config.time_step, embeddingsize)  # 256\n",
    "        self.naturelinear = Linear(naturesize + config.time_step, embeddingsize)  # 256\n",
    "        self.dynamiclinear = Linear(self.dynamic_size, embeddingsize)\n",
    "        self.xlinear = Linear(x_size, embeddingsize)\n",
    "        self.valuelinear = Linear(1, embeddingsize)\n",
    "        self.concat_downsample = Linear(embeddingsize * 3, embeddingsize)\n",
    "        self.conv = Conv1d(in_channels=config.time_step,\n",
    "                           out_channels=config.time_step,\n",
    "                           kernel_size=5,\n",
    "                           padding=3,\n",
    "                           groups=config.time_step)\n",
    "        self.relu = ReLU()\n",
    "        self.softmax = Softmax(dim=2)\n",
    "        self.config = config\n",
    "        self.value_down = Linear(embeddingsize * 2, embeddingsize)\n",
    "\n",
    "    def forward(self, x, human, nature):\n",
    "        # X：[B,5,3]  Human:[B,5,4+5]  Nature:[B,5,9+5]\n",
    "        # print(nature.shape)   x是送入Encoder的数据，包括油嘴油压和产量\n",
    "        dynamic = x[:, :, :self.dynamic_size]  # dynamic只取油嘴油压\n",
    "        value = x[:, :, -1].unsqueeze(-1)  # value只取产量\n",
    "        x = self.xlinear(x)  # 这里分别处理主要是为了消融实验效果\n",
    "        value = self.valuelinear(value)\n",
    "        dynamic = self.dynamiclinear(dynamic)\n",
    "        if self.config.only_dynamic == 1:\n",
    "            return x  # 只用动态数据\n",
    "        res = dynamic  # only_dynamic only_static_concat_dynamic only_static_plus_dynamic\n",
    "        batch, _, xc = x.shape\n",
    "        # print(human.shape)      #动静态特征融合\n",
    "        human = self.humanlinear(human)\n",
    "        human = human[:batch, :, :]\n",
    "        nature = self.naturelinear(nature)\n",
    "        nature = nature[:batch, :, :]\n",
    "        if self.config.only_static_plus_dynamic == 1:\n",
    "            return x + human + nature  # 动静态相加\n",
    "        hn = torch.cat([human, nature], 2)  # 拼接静态特征\n",
    "        hn = hn[:batch, :, :]\n",
    "        xhn = torch.cat([dynamic, hn], 2)  # 动静态特征拼接\n",
    "        if self.config.only_static_concat_dynamic == 1:\n",
    "            return self.concat_downsample(xhn)  # 动静态拼接c\n",
    "        b, n, c = xhn.shape\n",
    "        Mask = torch.zeros(b, n, c).to(device=x.device)\n",
    "        xx = torch.zeros(b, n, c).to(device=x.device)\n",
    "        # 以一下部分为动静态融合\n",
    "        for i, j in enumerate(self.shuffle_list):\n",
    "            Mask[:, :, i] = xhn[:, :, j]  # 动静态特征shuffle\n",
    "\n",
    "        Mask = self.conv(Mask)  # Shuffle后进行卷积\n",
    "\n",
    "        for i, j in enumerate(self.shuffle_list):\n",
    "            xx[:, :, j] = Mask[:, :, i]  # 反shuffle\n",
    "        xx = self.relu(xx)\n",
    "        x_select = xx[:, :, :xc]  # 提取动态特征对应index部分的融合特征\n",
    "\n",
    "        # x_res =torch.mul(res,self.softmax(res))\n",
    "        x_select = self.softmax(x_select)  # 对融合特征计算注意力权重\n",
    "        dynamic = torch.mul(dynamic, x_select)\n",
    "        dynamic = (dynamic + res) / 2  # 保留残差结构\n",
    "        value = torch.cat([dynamic, value], 2)  # 将动态特征和产量信息拼接\n",
    "        value = self.value_down(value)\n",
    "        # x = x+res\n",
    "        return (value)\n",
    "\n",
    "    '''\n",
    "    动静态特征融合模块，输入为静态数据和5天的动态数据\n",
    "    输出按照parser的选择输出不同融合方式的动态数据\n",
    "    '''\n",
    "\n",
    "\n",
    "class Get_h_c(Module):  # 静态嵌入模块\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        inlength = config.humansize + config.naturesize\n",
    "        embedding = config.embedding_size\n",
    "\n",
    "        self.fc1 = Linear(inlength, embedding)\n",
    "        self.relu = ReLU()\n",
    "        self.LN1 = LayerNorm(embedding)\n",
    "        self.fc2 = Linear(embedding, 3 * embedding)\n",
    "        self.LN2 = LayerNorm(3 * embedding)\n",
    "\n",
    "        self.linearh0 = Linear(3 * embedding, embedding)\n",
    "        self.linearc0 = Linear(3 * embedding, embedding)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, human, nature):\n",
    "        # human(b,5,4+5) nature(b,5,9+5)这里的静态信息已经在Data类中提前处理了，添加了时间信息\n",
    "        human = human[:, :self.config.lstm_layers, :self.config.humansize]  # 这里剥离时间信息，只保留原始特征信息，并且保证维度和lstm层数相同\n",
    "        nature = nature[:, :self.config.lstm_layers, :self.config.naturesize]\n",
    "        # print(human.shape)\n",
    "        static = torch.cat([human, nature], dim=2)\n",
    "        out = self.fc1(static)\n",
    "        out = self.LN1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.LN2(out)\n",
    "        out = self.relu(out)\n",
    "        h_0 = self.linearh0(out)\n",
    "        c_0 = self.linearc0(out)\n",
    "        return h_0, c_0  # 大小为[B,2,256]\n",
    "\n",
    "    '''\n",
    "    静态嵌入模块，输入为静态数据，\n",
    "    输出为初始隐藏层h和细胞状态c\n",
    "    '''\n",
    "\n",
    "\n",
    "class Net(Module):  # 基于Seq2Seq的大结构\n",
    "    def __init__(self, config, teacherforcing=False, transfer=False):\n",
    "        super(Net, self).__init__()\n",
    "        # humansize = 4\n",
    "        # naturesize = 9\n",
    "        self.encoder = Encoder(input_dim=config.embedding_size, hidden_dim=config.hidden_size,\n",
    "                               num_layers=config.lstm_layers, dropout_rate=config.dropout_rate)\n",
    "        if config.use_attention == True:\n",
    "            self.decoder = ATT_Decoder(enc_hidden_size=config.hidden_size, dec_hidden_size=config.hidden_size,\n",
    "                                       num_layers=config.lstm_layers,\n",
    "                                       attention=True, config=config,\n",
    "                                       embedding_dim=config.embedding_size, dropout_rate=config.dropout_rate,\n",
    "                                       dynamicDe_num=config.dynamic_length)\n",
    "        else:\n",
    "            self.decoder = No_ATT_Decoder(enc_hidden_size=config.hidden_size, dec_hidden_size=config.hidden_size,\n",
    "                                          num_layers=config.lstm_layers,\n",
    "                                          attention=None,\n",
    "                                          embedding_dim=config.embedding_size, dropout_rate=config.dropout_rate,\n",
    "                                          dynamicDe_num=config.dynamic_length)\n",
    "\n",
    "        self.linear = Linear(in_features=config.hidden_size, out_features=config.output_size)\n",
    "        self.ECA = ECA(config)\n",
    "        self.teacherforcing = teacherforcing\n",
    "        self.Gethc = Get_h_c(config)\n",
    "        self.hc_initialize = config.use_static_embedding\n",
    "        self.transfer = transfer\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input, human, nature, h_0, c_0):\n",
    "        #  h_0和c_0用静态数据升维度的特征矩阵。 encoder_input：[B,5,3] decoder_input:[3,B,1] dyanmic_dec_input:[3,B,2]\n",
    "        # human,nature分别表示静态特征     human:[b,5,4+5=9]  nature:[b,5,9+5=14]\n",
    "        # print(human.shape,nature.shape)\n",
    "        if self.hc_initialize:\n",
    "            h_0, c_0 = self.Gethc(human, nature)\n",
    "            h_0 = h_0.permute(1, 0, 2).contiguous()\n",
    "            c_0 = c_0.permute(1, 0, 2).contiguous()\n",
    "            embedding_h = h_0\n",
    "\n",
    "        encoder_input = self.ECA(encoder_input, human, nature)  # 动静态特征融合\n",
    "\n",
    "        batch_size = encoder_input.shape[0]\n",
    "        predict_future_num = decoder_input.shape[0]  # 预测的时间长度\n",
    "\n",
    "        enc_outputs, h1, c1 = self.encoder(encoder_input, h_0, c_0)  # 编码器对5天数据进行编码\n",
    "        h2, c2 = h1.clone(), c1.clone()\n",
    "        # 获得Encoder特征矩阵  torch.Size([B, 5, 256]) torch.Size([2, B, 256]) torch.Size([2, B, 256]) 2层LSTM\n",
    "        encoder_transfer_h = h1\n",
    "        outs = torch.zeros(predict_future_num, batch_size, 1).to(\n",
    "            encoder_input.device)  # 创建outputs张量存储decoder输出，用于存放预测结果\n",
    "        dec_input = decoder_input[0, :, :].unsqueeze(0)\n",
    "        # dyna_dec_in = dyanmic_dec_input[0,:,:].unsqueeze(0)\n",
    "\n",
    "        for t in range(0, predict_future_num):  # 对于设定未来预测天数进行迭代\n",
    "            # dec_input = decoder_input[t,:,:]\n",
    "            if self.config.use_attention == True:\n",
    "                out, h1, c1, h2, c2 = self.decoder(dec_input, h1, c1, h2, c2, enc_outputs)\n",
    "            else:\n",
    "                out, h1, c1 = self.decoder(dec_input, h1, c1, enc_outputs)\n",
    "            outs[t] = out\n",
    "\n",
    "            # dyna_dec_in = dyanmic_dec_input[t,:,:].unsqueeze(0)\n",
    "            # 可取真实产量值作为下一时刻的输入，也可取这一次的预测值作为下一时刻的hc输入\n",
    "            # 在于teacherforcing训练方式的与否（目前不采用Teacherforcing的训练方式）\n",
    "            # 在测试时候Teacherforcing 必须设置为False\n",
    "            dec_input = decoder_input[t, :, :].unsqueeze(0) if self.teacherforcing else out\n",
    "        if self.transfer == True:\n",
    "\n",
    "            return outs, (encoder_transfer_h, embedding_h)\n",
    "        else:\n",
    "            return outs\n",
    "\n",
    "    '''\n",
    "    整体模型\n",
    "    '''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:03:10 ] Epoch 0/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:05<00:00,  4.42it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 23.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:03:16 ] The train loss is 0.557217. The valid loss is 0.593311.\n",
      "[ 2023/03/01 15:03:16 ] Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.46it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 23.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:03:21 ] The train loss is 0.364367. The valid loss is 0.320396.\n",
      "[ 2023/03/01 15:03:21 ] Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.34it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 21.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:03:26 ] The train loss is 0.221556. The valid loss is 0.179419.\n",
      "[ 2023/03/01 15:03:26 ] Epoch 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.40it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 23.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:03:32 ] The train loss is 0.167426. The valid loss is 0.133682.\n",
      "[ 2023/03/01 15:03:32 ] Epoch 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.57it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 22.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:03:36 ] The train loss is 0.137561. The valid loss is 0.094928.\n",
      "[ 2023/03/01 15:03:36 ] Epoch 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.53it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 23.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:03:41 ] The train loss is 0.114893. The valid loss is 0.085695.\n",
      "[ 2023/03/01 15:03:41 ] Epoch 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.49it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 24.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:03:46 ] The train loss is 0.105359. The valid loss is 0.080280.\n",
      "[ 2023/03/01 15:03:46 ] Epoch 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.53it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 22.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:03:51 ] The train loss is 0.103734. The valid loss is 0.077402.\n",
      "[ 2023/03/01 15:03:51 ] Epoch 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.46it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 22.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:03:56 ] The train loss is 0.099624. The valid loss is 0.077991.\n",
      "[ 2023/03/01 15:03:56 ] Epoch 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.52it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 23.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:04:01 ] The train loss is 0.097227. The valid loss is 0.073044.\n",
      "[ 2023/03/01 15:04:01 ] Epoch 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.55it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 23.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:04:06 ] The train loss is 0.094816. The valid loss is 0.074674.\n",
      "[ 2023/03/01 15:04:06 ] Epoch 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.52it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 23.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:04:11 ] The train loss is 0.094673. The valid loss is 0.073832.\n",
      "[ 2023/03/01 15:04:11 ] Epoch 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.53it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 23.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:04:16 ] The train loss is 0.093556. The valid loss is 0.071818.\n",
      "[ 2023/03/01 15:04:16 ] Epoch 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.58it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 22.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:04:21 ] The train loss is 0.091672. The valid loss is 0.073250.\n",
      "[ 2023/03/01 15:04:21 ] Epoch 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.48it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 21.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:04:26 ] The train loss is 0.092873. The valid loss is 0.078278.\n",
      "[ 2023/03/01 15:04:26 ] Epoch 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.60it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 23.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:04:31 ] The train loss is 0.091931. The valid loss is 0.086263.\n",
      "[ 2023/03/01 15:04:31 ] Epoch 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.59it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 23.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:04:36 ] The train loss is 0.090660. The valid loss is 0.075544.\n",
      "[ 2023/03/01 15:04:36 ] Epoch 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 26/26 [00:04<00:00,  5.58it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 24.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2023/03/01 15:04:41 ] The train loss is 0.089831. The valid loss is 0.074965.\n",
      "[ 2023/03/01 15:04:41 ]  The training stops early in epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if config.do_train:\n",
    "\n",
    "    model = Net(config, teacherforcing=False).to(device)  # 如果是GPU训练， .to(device) 会把模型/数据复制到GPU显存中\n",
    "    if config.add_train:  # 如果是增量训练，会先加载原模型参数\n",
    "        model.load_state_dict(torch.load(config.model_save_path + config.name + config.model_name))\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = RMSLEloss()  # 这两句是定义优化器和loss\n",
    "    # criterion = torch.nn.MSELoss()\n",
    "    valid_loss_min = float(\"inf\")\n",
    "    bad_epoch = 0\n",
    "    global_step = 0\n",
    "    for epoch in range(config.epoch):\n",
    "        logger.info(\"Epoch {}/{}\".format(epoch, config.epoch))\n",
    "        model.train()  # pytorch中，训练时要转换成训练模式\n",
    "        train_loss_array = []\n",
    "\n",
    "        h_0, c_0 = None, None\n",
    "        loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        for i, _data in loop:\n",
    "            setproctitle.setproctitle(\"zys:\" + str(epoch) + \"/\" + \"{}\".format(config.epoch))\n",
    "            _encoder_train, _decoder_train, label_train, human, nature = _data[0].to(device), _data[1].to(device), \\\n",
    "                                                                         _data[2].to(device), _data[3].to(device), \\\n",
    "                                                                         _data[4].to(device) ## 取数据\n",
    "            optimizer.zero_grad()  # 训练前要将梯度信息置 0\n",
    "            # print(_encoder_train.shape,_decoder_train.shape, label_train.shape)  #torch.Size([B, 5, 3]) torch.Size([B, 3, 1]) torch.Size([B, 3, 1])\n",
    "            _decoder_train = _decoder_train.permute(1, 0, 2)\n",
    "            label_train = label_train.permute(1, 0, 2)  # 成为[3,1(batch_size),1]\n",
    "            # dynamic_train = dynamic_train.permute(1,0,2) #成为[3,1(batch_size),2]\n",
    "\n",
    "            pred_Y = model(_encoder_train, _decoder_train, human, nature, h_0, c_0)\n",
    "\n",
    "            loss = criterion(pred_Y, label_train)  # 计算loss\n",
    "            loss.backward()  # 将loss反向传播\n",
    "            optimizer.step()\n",
    "            # 用优化器更新参数\n",
    "            train_loss_array.append(loss.item())\n",
    "            global_step += 1\n",
    "            if config.do_train_visualized and global_step % 100 == 0:  # 每一百步显示一次\n",
    "                vis.line(X=np.array([global_step]), Y=np.array([loss.item()]), win='Train_Loss',\n",
    "                         update='append' if global_step > 0 else None, name='Train', opts=dict(showlegend=True))\n",
    "        # 以下为验证集和早停机制，当模型训练连续config.patience个epoch都没有使验证集预测效果提升时，就停止，防止过拟合\n",
    "\n",
    "        model.eval()  # pytorch中，预测时要转换成预测模式\n",
    "        model.teacherforcing = False\n",
    "        valid_loss_array = []\n",
    "        h_0, c_0 = None, None\n",
    "        loopv = tqdm(enumerate(valid_loader), total=len(valid_loader))\n",
    "        for i, _data in loopv:\n",
    "            encoder_valid, decoder_valid, label_valid, human, nature = _data[0].to(device), _data[1].to(device), _data[\n",
    "                2].to(device), _data[3].to(device), _data[4].to(device)\n",
    "            decoder_valid = decoder_valid.permute(1, 0, 2)\n",
    "            label_valid = label_valid.permute(1, 0, 2)\n",
    "            # dynamic_valid = dynamic_valid.permute(1,0,2)\n",
    "            # _valid_X, _encoder_valid = _valid_X.to(device), _encoder_valid.to(device)\n",
    "            pred_Y = model(encoder_valid, decoder_valid, human, nature, h_0, c_0)\n",
    "\n",
    "            loss = criterion(pred_Y, label_valid)  # 验证过程只有前向计算，无反向传播过程\n",
    "            valid_loss_array.append(loss.item())\n",
    "\n",
    "        train_loss_cur = np.mean(train_loss_array)\n",
    "        valid_loss_cur = np.mean(valid_loss_array)\n",
    "        logger.info(\"The train loss is {:.6f}. \".format(train_loss_cur) +\n",
    "                    \"The valid loss is {:.6f}.\".format(valid_loss_cur))\n",
    "\n",
    "        if valid_loss_cur < valid_loss_min:\n",
    "            valid_loss_min = valid_loss_cur\n",
    "            bad_epoch = 0\n",
    "            torch.save(model.state_dict(), config.model_save_path + config.name + config.model_name)  # 模型保存\n",
    "        else:\n",
    "            bad_epoch += 1\n",
    "            if bad_epoch >= config.patience:  # 如果验证集指标连续patience个epoch没有提升，就停掉训练\n",
    "                logger.info(\" The training stops early in epoch {}\".format(epoch))\n",
    "                break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.模型预测"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1 数据预处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "config.do_predict = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "torch.Size([8, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "if config.do_predict:\n",
    "    config.batch_size = 1\n",
    "    # 获取测试集\n",
    "    encoder_test, human, nature = data_gainer.get_test_data(roll=False, return_label_data=False)\n",
    "    # pred_result = predict(config, encoder_test, human, nature)  # 这里输出的是未还原的归一化预测数据\n",
    "        # 获取测试数据\n",
    "    device = torch.device(config.cudadevice if config.use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "    encoder_test = encoder_test\n",
    "    encoder_test = torch.from_numpy(encoder_test).float()\n",
    "    # dynamic_test =dynamic_test\n",
    "    # dynamic_test = torch.from_numpy(dynamic_test).float()\n",
    "    print(encoder_test.shape)\n",
    "    test_loader = DataLoader(TensorDataset(encoder_test), batch_size=1)\n",
    "\n",
    "    human, nature = torch.from_numpy(human).float(), torch.from_numpy(nature).float()\n",
    "    bathsize = config.batch_size\n",
    "\n",
    "    human = human.repeat(bathsize, 1, 1).to(device)\n",
    "    nature = nature.repeat(bathsize, 1, 1).to(device)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2模型测试模块"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5, 3])\n",
      "(26, 1)\n",
      "[ 2023/03/01 15:05:31 ] The mean relative error of ['ri-chan-you-liang'] is [0.03913086]\n",
      "[ 2023/03/01 15:05:31 ] The mean squared error of ['ri-chan-you-liang'] is 0.3220409820460611\n",
      "[ 2023/03/01 15:05:31 ] The mean average error of ['ri-chan-you-liang'] is 0.5044259084121118\n",
      "[ 2023/03/01 15:05:31 ] The R^2 of ['ri-chan-you-liang'] is -3.1672556655608215\n",
      "[ 2023/03/01 15:05:31 ] The predicted ri-chan-you-liang for the next 3 day(s) is: [13.05201962 12.90388551 13.08596773]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if config.do_predict:\n",
    "\n",
    "    # 加载模型\n",
    "    model = Net(config, teacherforcing=False).to(device)\n",
    "    model.load_state_dict(torch.load(config.model_save_path + config.name + config.model_name))  # 加载模型参数\n",
    "\n",
    "    # 先定义一个tensor保存预测结果\n",
    "    result = torch.Tensor().to(device)\n",
    "    predict_day = config.predict_day\n",
    "    # 预测过程\n",
    "    model.eval()\n",
    "    h_0, c_0 = None, None\n",
    "    print(encoder_test.shape)\n",
    "    for _data in test_loader:\n",
    "        data_X = _data[0].to(device)\n",
    "        decoder_input0 = data_X[:, -1:, -1:].permute(1, 0, 2)  # 取输入特征的最后一维作为Decoder0的初始输入\n",
    "        # 因为Predict所以这么设计\n",
    "        predictTensor = torch.zeros(predict_day - 1, 1, 1).to(device)\n",
    "        decoder_input = torch.cat((decoder_input0, predictTensor), dim=0)\n",
    "        # dynamic_input = dynamic_test.permute(1,0,2)\n",
    "\n",
    "        pred_X = model(data_X, decoder_input, human, nature, h_0, c_0)\n",
    "        cur_pred = torch.squeeze(pred_X, dim=2)\n",
    "        result = torch.cat((result, cur_pred), dim=0)\n",
    "        pred_result = result.detach().cpu().numpy() # 先去梯度信息，如果在gpu要转到cpu，最后要返回numpy数据\n",
    "    draw(config, data_gainer, logger, pred_result, '_no_roll')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    # argparse方便于命令行下输入参数，可以根据需要增加更多\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # 如果在linux下面, 可以方便采用下面的参数输入\n",
    "    # 在Windows环境下，可以单独修改参数，直接运行\n",
    "    parser.add_argument(\"-b\", \"--batch_size\", default=16, type=int, help=\"batch size\")\n",
    "    parser.add_argument(\"-e\", \"--epoch\", default=100, type=int, help=\"epochs num\")\n",
    "    parser.add_argument(\"--ttotrain\", default=0, type=int, help=\"1代表训练,0代表测试\")\n",
    "    parser.add_argument(\"--use_attention\", default=1, type=int, help=\"seq2seq_attention\")\n",
    "    parser.add_argument(\"--use_static_embedding\", default=1, type=int, help=\"Inite,h0_c0\")\n",
    "\n",
    "    parser.add_argument(\"--only_dynamic\", default=0, type=int, help=\"只用动态嵌入\")\n",
    "    parser.add_argument(\"--only_static_concat_dynamic\", default=0, type=int, help=\"静态拼接动态降维\")\n",
    "    parser.add_argument(\"--only_static_plus_dynamic\", default=0, type=int, help=\"静态和动态直接相加\")\n",
    "\n",
    "    parser.add_argument(\"--roll_predict_day\", default=125, type=int, help=\"迭代预测天数\")  # 相当于让Decoder迭代多少次\n",
    "\n",
    "    parser.add_argument(\"--predict_day\", default=3, type=int, help=\"Decoder预测天数\")\n",
    "    parser.add_argument(\"--time_step\", default=5, type=int, help=\"Encoder读取天数\")\n",
    "\n",
    "    parser.add_argument(\"--name\", default='A1', type=str, help=\"填写训练预设名称\")  # 可以填写'All' 或者'A1,B1,B590_1,B123'这种形式\n",
    "    parser.add_argument(\"--val_name\", default='C1', type=str, help=\"填写测试预设名称\")  # 填写'C1'这种形式，暂时只支持测试1口井\n",
    "    # only_dynamic only_static_concat_dynamic only_static_plus_dynamic\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    con = Config()\n",
    "    for key in dir(args):  # dir(args) 函数获得args所有的属性\n",
    "        if not key.startswith(\"_\"):  # 去掉 args 自带属性，比如__name__等\n",
    "            setattr(con, key, getattr(args, key))  # 将属性值赋给Config\n",
    "    # 建议window下，采用下面, 便于调试\n",
    "\n",
    "    assert con.only_dynamic + con.only_static_concat_dynamic + con.only_static_plus_dynamic <= 1\n",
    "    # 这几个参数做消融实验时候只能有同时存才1个或者都不存在，就代表动静态融合模块\n",
    "    sign = True if con.ttotrain == 1 else False\n",
    "    use_attention = '_No_Attention'\n",
    "    use_static_embedding = '_hc_False'\n",
    "    dynamic_fusion = '_ds_fusion'\n",
    "    if con.only_dynamic == 1:\n",
    "        dynamic_fusion = '_dynamic_only'\n",
    "    if con.only_static_concat_dynamic == 1:\n",
    "        dynamic_fusion = '_concat_only'\n",
    "    if con.only_static_plus_dynamic == 1:\n",
    "        dynamic_fusion = '_plus_only'\n",
    "    if con.use_attention == 1:\n",
    "        con.use_attention = True\n",
    "        use_attention = '_Attention'\n",
    "    else:\n",
    "        con.use_attention = False\n",
    "    if con.use_static_embedding == 1:\n",
    "        con.use_static_embedding = True\n",
    "        use_static_embedding = '_hc_True'\n",
    "    else:\n",
    "        con.use_static_embedding = False\n",
    "    con.train_name = con.name\n",
    "    end_sign = '_RMLSE' + str(con.batch_size)\n",
    "    con.name = con.name + use_attention + use_static_embedding + end_sign + dynamic_fusion + str(con.time_step) + str(\n",
    "        con.predict_day)\n",
    "    # 设定名称，用于指示使用方法以及数据\n",
    "    print('Train:' + str(sign))\n",
    "\n",
    "    if sign == True:  # 表示训练\n",
    "        if ',' in con.train_name:\n",
    "            con.data_selected = con.train_name.split(',')\n",
    "            print('已选择{}井训练'.format(con.data_selected))\n",
    "        elif con.train_name == 'All':\n",
    "            con.data_selected = ['All']\n",
    "            print('已选择[{}]路径下所有井训练'.format(con.dynamic_data_root))\n",
    "        else:\n",
    "            con.data_selected = [con.train_name]  # 只输入单井情况\n",
    "            print('已选择{}井训练'.format(con.data_selected))\n",
    "        if not con.train_name:\n",
    "            assert ('井名输入错误')\n",
    "\n",
    "        con.train_data_rate = 0.95  # 设定数据用于训练的百分比\n",
    "        con.add_train = False  # 不加载权重进行训练\n",
    "        con.do_train = True\n",
    "        con.do_predict = False\n",
    "        con.do_predict_roll = False\n",
    "    else:\n",
    "        con.data_selected = [con.val_name]\n",
    "        if not con.train_name:\n",
    "            assert ('井名输入错误')\n",
    "        con.train_data_rate = 0.1  # 取井剩下1-train_data_rate的百分比用于测试\n",
    "        con.do_train = False\n",
    "        con.do_predict = True\n",
    "        con.do_predict_roll = False\n",
    "\n",
    "    main(con)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}